{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "258151a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from robust_models.IRM_model.IRMClassifier import IRMClassifier\n",
    "from robust_models.DRO_model.DROClassifier import GroupDROClassifier\n",
    "from robust_models.DRO_model.AdversarialDRO import AdversarialLabelDRO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94006bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(true_labels, predicted_probs):\n",
    "    total_nll = 0.0\n",
    "    for i in range(len(true_labels)):\n",
    "        true_class = true_labels[i]\n",
    "        # Get predicted probability for the true class\n",
    "        prob_true_class = predicted_probs[i, true_class]\n",
    "        \n",
    "        # Avoid log(0) which would be infinity\n",
    "        if prob_true_class < 1e-15:\n",
    "            prob_true_class = 1e-15\n",
    "            \n",
    "        # Calculate negative log of the probability\n",
    "        sample_nll = -np.log(prob_true_class)\n",
    "        total_nll += sample_nll\n",
    "\n",
    "    \n",
    "    # Return average NLL\n",
    "    return total_nll / len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bff0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/electricity_source.csv', index_col=[0])\n",
    "data[list(data.columns)[-1]] = data[list(data.columns)[-1]].astype(int)\n",
    "X, y = data[list(data.columns)[:-1]].values, data[list(data.columns)[-1]].values\n",
    "data_target = pd.read_csv('../data/electricity_target.csv', index_col=[0])\n",
    "data_target[list(data_target.columns)[-1]] = data_target[list(data_target.columns)[-1]].astype(int)\n",
    "data_target.reset_index(inplace=True, drop=True)\n",
    "X_ood, y_ood = data_target[list(data_target.columns)[:-1]].values, data_target[list(data_target.columns)[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413f9108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347036463466142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AdversarialLabelDRO(\n",
    "    input_dim=X.shape[1],  # Use actual feature count\n",
    "    hidden_dims=[64, 32],\n",
    "    eta_pi=0.1,\n",
    "    r=0.1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y, epochs=20, batch_size=64)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_ood)\n",
    "f1_score(y_ood, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebb56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def generate_multi_domain_data(X, y, n_domains=2, method='random', random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into multiple domains using specified method.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (numpy array)\n",
    "        y: Target labels (numpy array)\n",
    "        n_domains: Number of domains to create\n",
    "        method: \n",
    "            'random' - random assignment\n",
    "            'kmeans' - clustering-based domains\n",
    "            'label_shift' - domains based on conditional label distribution p(y|x)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        X, y, domain_labels: Original features, labels, and generated domain labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    domain_labels = np.zeros(len(X), dtype=int)\n",
    "    \n",
    "    if method == 'random':\n",
    "        # Randomly assign each sample to a domain\n",
    "        domain_labels = np.random.randint(0, n_domains, size=len(X))\n",
    "    \n",
    "    elif method == 'kmeans':\n",
    "        # Use KMeans clustering to create domain boundaries\n",
    "        kmeans = KMeans(n_clusters=n_domains, random_state=random_state, n_init='auto')\n",
    "        domain_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # If we get fewer clusters than requested, assign randomly\n",
    "        unique_domains = np.unique(domain_labels)\n",
    "        if len(unique_domains) < n_domains:\n",
    "            missing = n_domains - len(unique_domains)\n",
    "            for i in range(missing):\n",
    "                domain_labels[np.random.choice(len(X))] = len(unique_domains) + i\n",
    "    \n",
    "    elif method == 'label_shift':\n",
    "        # Train a model to estimate p(y|x)\n",
    "        X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "            X, y, test_size=0.5, stratify=y, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Train simple logistic regression model\n",
    "        model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predicted probabilities on calibration set\n",
    "        probs = model.predict_proba(X_cal)\n",
    "        \n",
    "        # Compute confidence scores (max probability per sample)\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        \n",
    "        # Create domain boundaries based on confidence quantiles\n",
    "        quantiles = np.linspace(0, 1, n_domains + 1)\n",
    "        bin_edges = np.quantile(confidences, quantiles)\n",
    "        \n",
    "        # Assign domain labels to calibration set\n",
    "        cal_domains = np.digitize(confidences, bin_edges[1:-1], right=False)\n",
    "        \n",
    "        # Assign domain labels to full dataset\n",
    "        domain_labels = np.zeros(len(X), dtype=int) - 1  # Initialize with -1\n",
    "        \n",
    "        # Assign calibration set domains\n",
    "        domain_labels[X_cal.index if hasattr(X_cal, 'index') else \n",
    "                      np.arange(len(X))[len(X_train):]] = cal_domains\n",
    "        \n",
    "        # Assign training set to most similar domain\n",
    "        train_probs = model.predict_proba(X_train)\n",
    "        train_conf = np.max(train_probs, axis=1)\n",
    "        train_domains = np.digitize(train_conf, bin_edges[1:-1], right=False)\n",
    "        domain_labels[X_train.index if hasattr(X_train, 'index') else \n",
    "                     np.arange(len(X))[:len(X_train)]] = train_domains\n",
    "        \n",
    "        # Handle any unassigned samples (shouldn't occur, but safety)\n",
    "        unassigned = domain_labels == -1\n",
    "        if np.any(unassigned):\n",
    "            domain_labels[unassigned] = np.random.choice(n_domains, size=np.sum(unassigned))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"method must be 'random', 'kmeans', or 'label_shift'\")\n",
    "    \n",
    "    return X, y, domain_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25afa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Irina\\anaconda3\\envs\\synth_shift\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Irina\\anaconda3\\envs\\synth_shift\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Irina\\anaconda3\\envs\\synth_shift\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Irina\\anaconda3\\envs\\synth_shift\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Irina\\anaconda3\\envs\\synth_shift\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0: 9252 samples\n",
      "Domain 1: 1063 samples\n",
      "\n",
      "Test Accuracy: 0.6877\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "X, y, domains = generate_multi_domain_data(X, y, method='kmeans')\n",
    "\n",
    "\n",
    "# Create domain-specific DataLoaders\n",
    "domain_loaders = []\n",
    "for domain_id in np.unique(domains):\n",
    "    domain_mask = (domains == domain_id)\n",
    "    X_domain = X[domain_mask]\n",
    "    y_domain = y[domain_mask]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_domain, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_domain, dtype=torch.long)\n",
    "    \n",
    "    # Create DataLoader for this domain\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    domain_loaders.append(loader)\n",
    "    \n",
    "    print(f\"Domain {domain_id}: {len(X_domain)} samples\")\n",
    "\n",
    "# Create test DataLoader\n",
    "X_test_tensor = torch.tensor(X_ood, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_ood, dtype=torch.long)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_tensor, y_test_tensor),\n",
    "    batch_size=128,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Initialize IRM model\n",
    "model = IRMClassifier(\n",
    "    input_size=X.shape[1],\n",
    "    num_classes=len(np.unique(y)),\n",
    "    hidden_size=64,\n",
    "    dropout=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    irm_lambda=0.8,\n",
    "    irm_penalty_anneal_iters=500\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(domain_loaders, n_iterations=3000)\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model.predict(x_batch)\n",
    "        test_preds.extend(preds)\n",
    "        test_true.extend(y_batch.numpy())\n",
    "\n",
    "accuracy = f1_score(test_true, test_preds)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth_shift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
